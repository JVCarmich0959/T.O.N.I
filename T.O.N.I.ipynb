{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Idea\n",
        "\n",
        "While studying NLP an idea took root in my mind and began to blossom, unfolding in slow layers, and then displayed itself clearly.\n",
        "\n",
        "When I was 9 years old my Dad was obsessed with assimilating to the ideas of respectibility; pantomiming the idea of status, through the thoughts and ideas of others. The consequence of which was a ritual of surrender conducted after supper prayer before popular daytime television host, Oprah Winfrey. \n",
        "\n",
        "Anything that she endorsed was taken as a communinon of sorts,and somehow, In the 5th grade I visited the local library of my small hometown checking out a book my parent's personal messiah recommended on her book list. A fruit that I'm certain my conservative southern baptist parents served me unknowingly...  'The Bluest Eye' by Toni Morrison. \n",
        "\n",
        "Up until this point I had only read Harry Potter series,the *Red Wall* series by Brian Jacques, and some other childhood favorites (The Boxcar Children, Junie B. Jones, A Wrinkle in Time, Black Beauty...etc.) . The limits of what I could conceive were primarily  defined by castles, wizards, talking animals...(Home life was bleak and demanding and I was cloying for escape)\n",
        "\n",
        "However, reading this book, at the summit of girlhood and adolescence contextualized, personal truths that I had never been given, and that nobody cared  to (or maybe could?) share with me. Language became liberation. I was able to understand that I held value not just for how well I courted obedience, but for who I was.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e5cbWPQG87wO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install gTTS\n",
        "!pip install playsound\n",
        "!pip install pyaudio\n",
        "!apt-get install -y portaudio19-dev\n",
        "!sudo apt-get install -y xvfb x11-utils\n",
        "!sudo apt-get install -y x11vnc\n",
        "!sudo apt-get install -y fluxbox\n",
        "!sudo apt-get install -y ffmpeg\n",
        "!sudo apt-get install -y xterm\n",
        "!pip install gTTS\n",
        "!pip install playsound\n",
        "!pip install pyaudio\n",
        "!apt-get install -y portaudio19-dev\n",
        "\n",
        "\n",
        "\n",
        "# Start virtual display\n",
        "!Xvfb :1 -screen 0 1024x768x24 &> xvfb.log  &\n",
        "!export DISPLAY=:1\n",
        "\n",
        "# Start audio\n",
        "!pulseaudio --start\n",
        "\n",
        "# Test audio\n",
        "!paplay /usr/share/sounds/alsa/Front_Center.wav\n",
        "\n",
        "!pip install openai\n",
        "!pip install tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yVTi8IBCIAi",
        "outputId": "905958fb-f53b-424e-de68-9e3f4d0fbfa0"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 160, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 241, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 526, in run\n",
            "    env = get_environment(lib_locations)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/__init__.py\", line 85, in get_environment\n",
            "    return select_backend().Environment.from_paths(paths)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 232, in from_paths\n",
            "    return cls(pkg_resources.WorkingSet(paths))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 567, in __init__\n",
            "    self.add_entry(entry)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 623, in add_entry\n",
            "    for dist in find_distributions(entry, True):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2065, in find_on_path\n",
            "    for dist in factory(fullpath):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2134, in distributions_from_metadata\n",
            "    yield Distribution.from_location(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2585, in from_location\n",
            "    match = EGG_NAME(basename)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 214, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 197, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 169, in emit\n",
            "    renderable = self.render_message(record, message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/logging.py\", line 189, in render_message\n",
            "    message_text = highlighter(message_text)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/highlighter.py\", line 35, in __call__\n",
            "    highlight_text = text.copy()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/text.py\", line 427, in copy\n",
            "    def copy(self) -> \"Text\":\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 160, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 241, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 499, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 631, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3033, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in parse_requirements\n",
            "    yield Requirement(line)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3101, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4959, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 214, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 197, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1622, in _log\n",
            "    record = self.makeRecord(self.name, level, fn, lno, msg, args,\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1591, in makeRecord\n",
            "    rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 328, in __init__\n",
            "    self.msecs = int((ct - int(ct)) * 1000) + 0.0  # see gh-89047\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gTTS in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gTTS) (8.1.3)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gTTS) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: playsound in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyaudio in /usr/local/lib/python3.10/dist-packages (0.2.13)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "portaudio19-dev is already the newest version (19.6.0-1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+5).\n",
            "xvfb is already the newest version (2:1.20.13-1ubuntu1~20.04.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11vnc is already the newest version (0.9.16-3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fluxbox is already the newest version (1.3.5-2build2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xterm is already the newest version (353-1ubuntu1.20.04.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gTTS in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gTTS) (2.27.1)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gTTS) (8.1.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (1.26.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: playsound in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyaudio in /usr/local/lib/python3.10/dist-packages (0.2.13)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "portaudio19-dev is already the newest version (19.6.0-1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "/bin/bash: pulseaudio: command not found\n",
            "/bin/bash: paplay: command not found\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "ERROR: unknown command \"istall\" - maybe you meant \"install\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining a model function to fine-tune our custom model:\n",
        "\n",
        "* Intially the function takes four parameters: model_name, which specifies the name of the pre-trained GPT-2 model to use as a starting point; corpus_path, which is the path to a text file containing the custom corpus to use for fine-tuning; output_dir, which is the directory where the fine-tuned model and tokenizer will be saved; and epochs and batch_size, which specify the number of training epochs and the batch size to use for fine-tuning. \n",
        "\n",
        "* We then load the pre-trained GPT-2 model and tokenizer specified by model_name using the from_pretrained method provided by the Hugging Face Transformers library.\n",
        "\n",
        "* The function then loads the custom corpus from the corpus_path file and tokenizes it using the pre-trained tokenizer. This is done using the TextDataset class provided by the Hugging Face Datasets library (They think of EVERYTHING). The tokenized data is split into chunks of 128 tokens (or fewer, if the end of the sequence is reached), which is specified by the block_size parameter.\n",
        "\n",
        "* The function prepares a data collator using the DataCollatorForLanguageModeling class provided by the Hugging Face Transformers library. This data collator is used to prepare batches of data for training the fine-tuned model.\n",
        "\n",
        "* We then set up  our training arguments using the TrainingArguments class provided by the Hugging Face Transformers library. These arguments specify various training settings, such as the output directory for the fine-tuned model, the number of training epochs, the batch size, and the learning rate.\n",
        "\n",
        "* Then we launch a Trainer object using the Trainer class provided by the Hugging Face Transformers library. This trainer object is responsible for training the fine-tuned model using the specified dataset and training arguments.\n",
        "\n",
        "* Lastly we call the '.train()' method to fine-tune the model and save it to the directory\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3eqYoDP-VuQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the necessary libaries\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import requests\n",
        "from io import StringIO\n",
        "import speech_recognition as sr\n",
        "import pyttsx3\n",
        "import datetime\n",
        "import random\n",
        "import pyjokes\n",
        "import requests\n",
        "import json\n",
        "from gtts import gTTS\n",
        "import os\n",
        "import openai\n",
        "import pytz \n",
        "\n"
      ],
      "metadata": {
        "id": "WdWc9nz-B1Uq"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a function to fine tune a gpt2 model\n",
        "def fine_tune_gpt2(model_name, corpus_path, output_dir, epochs=10, batch_size=4):\n",
        "    # Load pre-trained model and tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name) # Load a pre-trained tokenizer\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name) # Load a pre-trained model\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=corpus_path,\n",
        "        block_size=128,\n",
        "    )\n",
        "\n",
        "    # Prepare data collator\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "\n",
        "  # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "      output_dir=output_dir,\n",
        "      overwrite_output_dir=True,\n",
        "      num_train_epochs=epochs,\n",
        "      per_device_train_batch_size=batch_size,\n",
        "      save_steps=10_000,\n",
        "      save_total_limit=4,\n",
        "      logging_dir='./logs',\n",
        "      learning_rate=2e-5, # a common range for the learning rate when fine-tuning generative transformers like GPT-2 is \n",
        "                          # between 1e-5 and 5e-5 so I start with 3e-5 and move smaller\n",
        ")\n",
        "\n",
        "    # Create Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "v58K0T20C7C0"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Corpus\n",
        "\n",
        "I extracted the text information from some of Toni Morrison's novels using the python library PyPDF2  and reading them to a text file. \n",
        "\n",
        "I DO NOT OWN THE RIGHTS TO ANY OF THESE.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4cq2t8Z1O3-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "corpus_url = \"https://raw.githubusercontent.com/JVCarmich0959/CSC228/main/Toni_corpus_.txt\"\n",
        "\n",
        "\n",
        "response = requests.get(corpus_url)\n",
        "corpus_text = response.text\n",
        "\n",
        "\n",
        "with open(\"temp.txt\", \"w\") as f:\n",
        "    f.write(corpus_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "Uy6uLbJ8EK1y"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning the Model \n",
        "\n",
        "This takes a while!"
      ],
      "metadata": {
        "id": "CEvEtP3eXfj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine tune GPT-2 model on a collection of texts from Toni Morrison\n",
        "model_name = \"gpt2-large\"  # You can also use \"gpt2-small\" or \"distilgpt2\" to reduce memory requirements.\n",
        "corpus_path = \"temp.txt\"\n",
        "output_dir = \"toni_morrison_gpt2\"\n",
        "fine_tune_gpt2(model_name, corpus_path, output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "id": "I2buZYMeDP5j",
        "outputId": "4065833e-ba14-4760-d49a-ff61eb54c38e"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12950' max='12950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12950/12950 54:40, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.314600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.194300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>2.954600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.745900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>2.734300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>2.417200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>2.363500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>2.293900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>2.016600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>2.046200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>1.852200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>1.759100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>1.749200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>1.509000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>1.530800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>1.450900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>1.331600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>1.343900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>1.212600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>1.212100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>1.170700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>1.101600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>1.097300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>1.057600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>1.038100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a refined model...it's time to generate some text...\n",
        "\n",
        "# Below I implement a class  named \"GPT2Generator\":\n",
        "\n",
        "* **__init__(self, model_path)**: This is the class constructor method that initializes an instance of the class. It takes a single argument model_path, which is the path to the directory containing the fine-tuned GPT-2 model and tokenizer.\n",
        "\n",
        "* **load_model(self, model_path)**: This method loads the fine-tuned GPT-2 model and tokenizer from the specified directory model_path. It returns a tuple containing the loaded model and tokenizer.\n",
        "\n",
        "* **set_model_to_eval(self, model)**: This method sets the mode of the loaded model to evaluation mode.\n",
        "\n",
        "* **generate_text(self, seed_text, num_words_to_generate=50, temperature=0.7)**: This is the main method of the class that generates text based on a given seed text. It takes three arguments: seed_text, which is the seed text for text generation, num_words_to_generate, which is the number of words to generate after the seed text, and temperature, which controls the degree of randomness in text generation. The method first tokenizes the seed text using the tokenizer, creates an attention mask, and generates text using the fine-tuned GPT-2 model. It then decodes the generated text and returns it."
      ],
      "metadata": {
        "id": "RMioOk2hXmOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class GPT2Generator:\n",
        "    def __init__(self, model_path):\n",
        "        self.model, self.tokenizer = self.load_model(model_path)\n",
        "        self.set_model_to_eval(self.model)\n",
        "    \n",
        "    def load_model(self, model_path):\n",
        "        model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "        return model, tokenizer\n",
        "    \n",
        "    def set_model_to_eval(self, model):\n",
        "        model.eval()\n",
        "    \n",
        "    def generate_text(self, seed_text, num_words_to_generate=50, temperature=0.7):\n",
        "        # Tokenize the seed text\n",
        "        input_ids = self.tokenizer.encode(seed_text, return_tensors=\"pt\")\n",
        "\n",
        "        # Create the attention mask\n",
        "        attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "        # Generate text\n",
        "        output = self.model.generate(\n",
        "            input_ids, \n",
        "            max_length=len(input_ids[0]) + num_words_to_generate, \n",
        "            num_return_sequences=1, # Change this to change the number of sentences to generate\n",
        "            no_repeat_ngram_size=2,  # To prevent repetition of bi-grams\n",
        "            do_sample=True, # To generate text randomly\n",
        "            temperature=temperature, # To change the degree of randomness\n",
        "            pad_token_id=self.tokenizer.eos_token_id,  # Set the pad_token_id\n",
        "            attention_mask=attention_mask,        # Pass the attention mask\n",
        "        )\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        return generated_text\n"
      ],
      "metadata": {
        "id": "6rqtUrz2IxxA"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a GPT2Generator object\n",
        "model_path = \"toni_morrison_gpt2\"\n",
        "gpt2_generator = GPT2Generator(model_path)\n",
        "\n",
        "# Generate text\n",
        "seed_text = \"She wailed at the top of her lungs \"  \n",
        "generated_text = gpt2_generator.generate_text(seed_text, num_words_to_generate=50, temperature=0.8)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOjrPiB7YVGi",
        "outputId": "4fc7fe63-5001-4ff0-cc2c-e277f58356f2"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She wailed at the top of her lungs  for what seemed like hours, her face a mask of pain she\n",
            "never imagined she would wear. Finally she gave up and went into the yard to lie on the\n",
            "grass. Her eyelids fell, but she thought she saw in the dark a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a GPT-2Generator object\n",
        "model_path = \"toni_morrison_gpt2\"\n",
        "gpt2_generator = GPT2Generator(model_path)\n",
        "\n",
        "# Prompt the model with a statement or question\n",
        "prompt = \"What do you think about the impact of literature on personal growth?\"\n",
        "\n",
        "# Generate a response in the style of Toni Morrison\n",
        "generated_text = gpt2_generator.generate_text(prompt, num_words_to_generate=50, temperature=0.8)\n",
        "\n",
        "# Print the response\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLV6sVWuWd_E",
        "outputId": "e35c90be-6af7-415b-d14c-8ab337888929"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What do you think about the impact of literature on personal growth?”\n",
            "The class was very diverse. A few had taken the\n",
            "college courses, most had not; most were old enough to have\n",
            "died twenty years ago; a few were just starting out. But the distribution\n",
            "of opinion was not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a GPT-2Generator object\n",
        "model_path = \"toni_morrison_gpt2\"\n",
        "gpt2_generator = GPT2Generator(model_path)\n",
        "\n",
        "# Prompt the model with a statement or question\n",
        "prompt = input(\"Enter your text: \")\n",
        "\n",
        "# Generate a response in the style of Toni Morrison\n",
        "generated_text = gpt2_generator.generate_text(prompt, num_words_to_generate=50, temperature=0.8)\n",
        "\n",
        "# Print the response\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD5tNnePQ-o6",
        "outputId": "4b703c83-08b0-46d8-d258-ddd2cb921098"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your text: Underneath the shade of a tree\n",
            "Underneath the shade of a tree, he said, “Is there a bathroom near here you don’t use?”\n",
            "“No.‘But I ll go if you want me to. It�s near my work. Can you tell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With its ability to generate human-like text on a wide range of topics, the custom fine-tuned GPT-2 model could be used to generate content for novels, screenplays, and other forms of creative writing. ( I didn't mean for this to be THAT relevant but...) This has the potential to revolutionize the creative writing process by providing writers with a powerful new tool that can help them generate ideas and overcome writer's block.\n",
        "\n",
        "The GPT-2 model can also be used for a wide range of NLP tasks, such as language translation, sentiment analysis, and even chatbot development. This makes it a powerful tool for automating tasks and generating responses in a human-like manner.\n",
        "\n",
        "One potential use case for the custom fine-tuned GPT-2 model could be in  the field of digital memorialization. By using existing text data such as emails, social media posts, and other written content, we can generate new content that captures the unique voice and personality of a deceased person. This is a powerful way to keep their memory alive and to create a lasting legacy.\n",
        "\n",
        "Another potential application of the custom fine-tuned GPT-2 model is customer service. With its ability to convey familiarity, the model could be used to generate responses to customer inquiries and complaints, freeing up human resources to focus on more complex issues.\n",
        "\n",
        "The model could also be used in the field of education to generate study materials and practice quizzes. This has the potential to revolutionize the way students learn and study, providing them with personalized and engaging content that is tailored to their individual needs."
      ],
      "metadata": {
        "id": "92hIrpvbUH-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More possibilities\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0mgK-j84R7cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tasks = {\n",
        "    \"time\": \"The current time is {time}\",\n",
        "    \"joke\": \"Here's a joke for you: \" + pyjokes.get_joke(),\n",
        "    \"weather\": \"The weather is currently {weather}\",\n",
        "    \"news\": \"Here are the latest headlines: {news}\"\n",
        "}\n",
        "\n",
        "def save_and_play_response(response_text, filename):\n",
        "    tts = gTTS(text=response_text, lang='en')\n",
        "    tts.save(filename)\n",
        "    os.system(f\"mpg321 {filename}\")\n",
        "\n",
        "# Define a function to perform predefined tasks based on input text\n",
        "def perform_task(input_text):\n",
        "    if \"goodbye\" in input_text.lower() or \"bye\" in input_text.lower():\n",
        "        return None\n",
        "\n",
        "    for task_name, task_response in tasks.items():\n",
        "        if task_name in input_text:\n",
        "            if task_name == \"time\":\n",
        "                # Get the timezone for Eastern US\n",
        "                eastern_tz = pytz.timezone('US/Eastern')\n",
        "                current_time = datetime.datetime.now(eastern_tz).strftime(\"%I:%M %p\")\n",
        "                task_response = task_response.format(time=current_time)\n",
        "            elif task_name == \"weather\":\n",
        "                response = requests.get(\"https://api.openweathermap.org/data/2.5/weather?q=Goldsboro,us&appid=3d627a0bded23d61f00d343a21807bf8\")\n",
        "                weather_data = json.loads(response.text)\n",
        "                weather_description = weather_data[\"weather\"][0][\"description\"]\n",
        "                task_response = task_response.format(weather=weather_description)\n",
        "            elif task_name == \"news\":\n",
        "                response = requests.get(\"https://newsapi.org/v2/top-headlines?country=us&apiKey=a5079ab552054b08bdb2b525c83adac6\")\n",
        "                news_data = json.loads(response.text)\n",
        "                news_headlines = [article[\"title\"] for article in news_data[\"articles\"]]\n",
        "                task_response = task_response.format(news=\"\\n\".join(news_headlines))\n",
        "            elif task_name == \"joke\":\n",
        "                task_response = \"Here's a joke for you: \" + pyjokes.get_joke()\n",
        "\n",
        "            # Save and play the response\n",
        "            save_and_play_response(task_response, \"task_response.mp3\")\n",
        "            return \"task_response.mp3\"\n",
        "\n",
        "    # If the input text doesn't match any predefined task, use GPT-3 to generate a response\n",
        "    else:\n",
        "        response_text = generate_response_gpt3(input_text)\n",
        "        save_and_play_response(response_text, \"gpt3_response.mp3\")\n",
        "        return \"gpt3_response.mp3\"\n",
        "\n",
        "\n",
        "def listen():\n",
        "    r = sr.Recognizer()\n",
        "    wake_word = \"toni\"\n",
        "\n",
        "    while True:\n",
        "        input_text = input(\"Enter a command: \")\n",
        "\n",
        "        if wake_word in input_text.lower():\n",
        "            print(\"Hey, I'm TONI! How can I help you?\")\n",
        "            break\n",
        "\n",
        "    input_text = input(\"Enter a command: \")\n",
        "    response_text = perform_task(input_text)\n",
        "    \n",
        "    while response_text is not None:\n",
        "        print(response_text)\n",
        "        input_text = input(\"Enter a command: \")\n",
        "        response_text = perform_task(input_text)\n",
        "\n",
        "    save_and_play_response(\"Goodbye! Have a great day!\", \"goodbye_response.mp3\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    listen()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfSADBIam7B8",
        "outputId": "2f3212be-d6db-40d2-e8fb-72bb5ba71fa7"
      },
      "execution_count": 126,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a command: Hi Toni\n",
            "Hey, I'm TONI! How can I help you?\n",
            "Enter a command: joke\n",
            "task_response.mp3\n",
            "Enter a command: Who is Toni Morrison?\n",
            "gpt3_response.mp3\n",
            "Enter a command: Where is Wayne Community College?\n",
            "gpt3_response.mp3\n",
            "Enter a command: bye\n"
          ]
        }
      ]
    }
  ]
}